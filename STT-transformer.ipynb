{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9107f25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:00.258302Z",
     "iopub.status.busy": "2025-10-04T10:01:00.257653Z",
     "iopub.status.idle": "2025-10-04T10:01:13.565779Z",
     "shell.execute_reply": "2025-10-04T10:01:13.564726Z",
     "shell.execute_reply.started": "2025-10-04T10:01:00.258272Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "%pip install Levenshtein\n",
    "import Levenshtein\n",
    "\n",
    "%config Completer.use_jedi = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd00511d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:13.568444Z",
     "iopub.status.busy": "2025-10-04T10:01:13.568073Z",
     "iopub.status.idle": "2025-10-04T10:01:13.576416Z",
     "shell.execute_reply": "2025-10-04T10:01:13.575629Z",
     "shell.execute_reply.started": "2025-10-04T10:01:13.568411Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class TextPreProcess:\n",
    "    \"\"\"Maps characters to integers and vice versa.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.vocab = [\"''\", \"<SPACE>\"] + list(string.ascii_lowercase)\n",
    "        self.char_map = {ch: i for i, ch in enumerate(self.vocab)}\n",
    "        self.index_map = {i: ch for ch, i in self.char_map.items()}\n",
    "        self.index_map[self.char_map[\"<SPACE>\"]] = \" \"\n",
    "\n",
    "    def text_to_int(self, text: str):\n",
    "        \"\"\"Convert text -> integer sequence.\"\"\"\n",
    "        return [self.char_map.get(ch, self.char_map[\"<SPACE>\"]) for ch in text.lower()]\n",
    "\n",
    "    def int_to_text(self, labels: list[int]):\n",
    "        \"\"\"Convert integer sequence -> text.\"\"\"\n",
    "        return \"\".join(self.index_map[i] for i in labels if i in self.index_map)\n",
    "\n",
    "    def int_to_text_remove_pad(self, labels: list[int]):\n",
    "        \"\"\"Remove trailing pad zeros, then convert.\"\"\"\n",
    "        while labels and labels[-1] == 0:\n",
    "            labels.pop()\n",
    "        return self.int_to_text(labels)\n",
    "\n",
    "text_transform = TextPreProcess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24415f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:13.578113Z",
     "iopub.status.busy": "2025-10-04T10:01:13.577631Z",
     "iopub.status.idle": "2025-10-04T10:01:13.594152Z",
     "shell.execute_reply": "2025-10-04T10:01:13.593545Z",
     "shell.execute_reply.started": "2025-10-04T10:01:13.578086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def levenshtein_distance(ref, hyp):\n",
    "    distance = Levenshtein.distance(ref, hyp)\n",
    "    return distance\n",
    "\n",
    "def normalize_text(text, ignore_case = False, remove_space = False):\n",
    "    if ignore_case:\n",
    "        text = text.lower()\n",
    "    if remove_space:\n",
    "        text = ''.join(text.split())\n",
    "    return text\n",
    "\n",
    "def calculate_errors(reference, hypothesis, ignore_case=False, remove_space=False, delimiter=None):\n",
    "    reference = normalize_text(reference, ignore_case, remove_space)\n",
    "    hypothesis = normalize_text(hypothesis, ignore_case, remove_space)\n",
    "\n",
    "    if delimiter:\n",
    "        reference = reference.split(delimiter)\n",
    "        hypothesis = hypothesis.split(delimiter)\n",
    "\n",
    "    edit_distance = levenshtein_distance(reference, hypothesis)\n",
    "    ref_len = len(reference)\n",
    "\n",
    "    return float(edit_distance), ref_len\n",
    "\n",
    "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
    "    edit_distance, ref_len = calculate_errors(reference, hypothesis, ignore_case, False, delimiter)\n",
    "    return edit_distance / ref_len\n",
    "\n",
    "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
    "    edit_distance, ref_len = calculate_errors(reference, hypothesis, ignore_case, remove_space)\n",
    "    return edit_distance / ref_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23401bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:13.596419Z",
     "iopub.status.busy": "2025-10-04T10:01:13.596102Z",
     "iopub.status.idle": "2025-10-04T10:01:13.799961Z",
     "shell.execute_reply": "2025-10-04T10:01:13.799089Z",
     "shell.execute_reply.started": "2025-10-04T10:01:13.596400Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_audio_transforms = nn.Sequential(\n",
    "    torchaudio.transforms.MelSpectrogram(sample_rate = 16000, n_mels = 128),\n",
    "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
    "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
    ")\n",
    "\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "\n",
    "pipeline_params = {\n",
    "    'batch_size': 10,\n",
    "    'epochs': 1,\n",
    "    'learning_rate': 5e-4,\n",
    "    'n_cnn_layers': 3, \n",
    "    'n_rnn_layers': 5,\n",
    "    'rnn_dim': 512,\n",
    "    'n_class': 29,\n",
    "    'n_feats': 128,\n",
    "    'stride': 2,\n",
    "    'dropout': 0.1,\n",
    "    'n_heads': 8,\n",
    "    'n_transformer_layers': 2,\n",
    "    'transformer_dim': 512\n",
    "}\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea776f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:13.801208Z",
     "iopub.status.busy": "2025-10-04T10:01:13.800925Z",
     "iopub.status.idle": "2025-10-04T10:01:13.809152Z",
     "shell.execute_reply": "2025-10-04T10:01:13.808384Z",
     "shell.execute_reply.started": "2025-10-04T10:01:13.801162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def data_processing(data, data_type = 'train'):\n",
    "    spectrograms = []\n",
    "    labels = []\n",
    "    input_lengths = []\n",
    "    label_lengths = []\n",
    "    \n",
    "    audio_transforms = train_audio_transforms if data_type == 'train' else valid_audio_transforms\n",
    "    \n",
    "    for (waveform, _, utterance, _, _, _) in data:\n",
    "        spec = audio_transforms(waveform).squeeze(0).transpose(0, 1) # (T, 128)\n",
    "        spectrograms.append(spec)\n",
    "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
    "        labels.append(label)\n",
    "        input_lengths.append(spec.shape[0] // 2)\n",
    "        label_lengths.append(len(label))\n",
    "        \n",
    "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first = True).unsqueeze(1).transpose(2,3)\n",
    "    labels = nn.utils.rnn.pad_sequence(labels, batch_first = True)\n",
    "    return spectrograms, labels, input_lengths, label_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970dd9ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:13.810871Z",
     "iopub.status.busy": "2025-10-04T10:01:13.810577Z",
     "iopub.status.idle": "2025-10-04T10:01:13.830282Z",
     "shell.execute_reply": "2025-10-04T10:01:13.829499Z",
     "shell.execute_reply.started": "2025-10-04T10:01:13.810852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CNNLayerNorm(nn.Module):\n",
    "    def __init__(self, n_feats):\n",
    "        super(CNNLayerNorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BATCH * CHANNEL * FEATURE * TIME\n",
    "        x = x.transpose(2, 3).contiguous()\n",
    "        x = self.layernorm(x)\n",
    "        return x.transpose(2, 3).contiguous()\n",
    "\n",
    "class ResidualCNN(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
    "        super(ResidualCNN, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel,\n",
    "            stride=stride,\n",
    "            padding=kernel//2,\n",
    "        )\n",
    "        self.cnn2 = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel,\n",
    "            stride=stride,\n",
    "            padding=kernel//2,\n",
    "        )\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
    "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.cnn1(x)\n",
    "        x = self.layer_norm2(x)\n",
    "        x = F.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.cnn2(x)\n",
    "        x += residual\n",
    "        return x\n",
    "\n",
    "class BidirectionalGRU(nn.Module):\n",
    "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
    "        super(BidirectionalGRU, self).__init__()\n",
    "        self.BiGRU = nn.GRU(\n",
    "            input_size=rnn_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            batch_first=batch_first,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = F.gelu(x)\n",
    "        x, _ = self.BiGRU(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3d54f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:13.831575Z",
     "iopub.status.busy": "2025-10-04T10:01:13.831356Z",
     "iopub.status.idle": "2025-10-04T10:01:13.847252Z",
     "shell.execute_reply": "2025-10-04T10:01:13.846458Z",
     "shell.execute_reply.started": "2025-10-04T10:01:13.831557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SelfAttentionHead(nn.Module):\n",
    "  \"\"\"\n",
    "  This is the self attention head\n",
    "  We want to be able to concat a bunch of these in the same dimension (unlike stack) to produce MultiHeadAttention\n",
    "  \"\"\"\n",
    "  def __init__(self, head_size, n_embd, dropout):\n",
    "    # technically head_size = n_embd / n_heads\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "    self.n_embd = n_embd\n",
    "\n",
    "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "  def forward(self, param):\n",
    "    # param = batch_size * tokens * embedding length [B * T * C]\n",
    "    # C = count of how many numbers represent each token. [\"Hello\", \"World\"] -> [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n",
    "    B, T, C = param.shape\n",
    "    key = self.key(param)\n",
    "    query = self.query(param)\n",
    "    value = self.value(param)\n",
    "\n",
    "    # now time for the maths formula\n",
    "    tmp = query @ key.transpose(-2, -1) * key.shape[-1] ** -0.5\n",
    "    \n",
    "    # Create causal mask dynamically based on actual sequence length\n",
    "    tril = torch.tril(torch.ones(T, T, device=param.device))\n",
    "    tmp = tmp.masked_fill(tril == 0, float('-inf'))\n",
    "    tmp = F.softmax(tmp, dim=-1)\n",
    "    tmp = self.dropout(tmp)\n",
    "\n",
    "    output = tmp @ value\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b2328-7d23-443e-abb7-40d06991314f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:13.848685Z",
     "iopub.status.busy": "2025-10-04T10:01:13.848431Z",
     "iopub.status.idle": "2025-10-04T10:01:13.864519Z",
     "shell.execute_reply": "2025-10-04T10:01:13.863651Z",
     "shell.execute_reply.started": "2025-10-04T10:01:13.848667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "  \"\"\"\n",
    "  Multihead attention block, needs params num_heads to define how many\n",
    "  SelfAttentionHead we actually need.\n",
    "  Concatenating all of their values\n",
    "  Finallu projecting them linearly back into the n_embd size so that they can be used to propagate further\n",
    "  \"\"\"\n",
    "  def __init__(self, num_heads, head_size, n_embd, dropout):\n",
    "    super().__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.head_size = head_size\n",
    "    self.n_embd = n_embd\n",
    "    \n",
    "    self.heads = nn.ModuleList([SelfAttentionHead(head_size, n_embd, dropout) for _ in range(num_heads)])\n",
    "    self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self, param):\n",
    "    output = torch.cat([h(param) for h in self.heads], dim=-1)\n",
    "    return self.dropout(self.proj(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018f4f28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:13.865932Z",
     "iopub.status.busy": "2025-10-04T10:01:13.865679Z",
     "iopub.status.idle": "2025-10-04T10:01:13.884295Z",
     "shell.execute_reply": "2025-10-04T10:01:13.883467Z",
     "shell.execute_reply.started": "2025-10-04T10:01:13.865914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Transformer-Enhanced Speech Recognition Model\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, \n",
    "                 stride=2, dropout=0.1, n_heads=8, n_transformer_layers=2, transformer_dim=512):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        n_feats = n_feats // 2\n",
    "        \n",
    "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)\n",
    "        self.rescnn_layers = nn.Sequential(*[\n",
    "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
    "            for _ in range(n_cnn_layers)\n",
    "        ])\n",
    "\n",
    "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
    "        \n",
    "        self.birnn_layers = nn.Sequential(*[\n",
    "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
    "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
    "            for i in range(n_rnn_layers)\n",
    "        ])\n",
    "\n",
    "        self.transformer_projection = nn.Linear(rnn_dim*2, transformer_dim)\n",
    "        \n",
    "        head_size = transformer_dim // n_heads\n",
    "        self.multi_head_attention_block = MultiHeadAttention(\n",
    "            num_heads=n_heads, \n",
    "            head_size=head_size, \n",
    "            n_embd=transformer_dim, \n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.transformer_layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=transformer_dim,\n",
    "                nhead=n_heads,\n",
    "                dim_feedforward=transformer_dim*4,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            for _ in range(n_transformer_layers)\n",
    "        ])\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(transformer_dim, rnn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(rnn_dim, n_class)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.rescnn_layers(x)\n",
    "        \n",
    "        B, C, F, T = x.size()\n",
    "        x = x.view(B, C*F, T)  # B*F*T\n",
    "        x = x.transpose(1, 2)  # B*T*F\n",
    "        x = self.fully_connected(x)\n",
    "        \n",
    "        x = self.birnn_layers(x)  # Shape: (B, T, rnn_dim*2)\n",
    "        \n",
    "        x = self.transformer_projection(x)  # Shape: (B, T, transformer_dim)\n",
    "        \n",
    "        x = self.multi_head_attention_block(x)  # Shape: (B, T, transformer_dim)\n",
    "        \n",
    "        for transformer_layer in self.transformer_layers:\n",
    "            x = transformer_layer(x)  # Shape: (B, T, transformer_dim)\n",
    "        \n",
    "        x = self.classifier(x)  # Shape: (B, T, n_class)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a562c6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:14.619931Z",
     "iopub.status.busy": "2025-10-04T10:01:14.619589Z",
     "iopub.status.idle": "2025-10-04T10:01:14.629069Z",
     "shell.execute_reply": "2025-10-04T10:01:14.628138Z",
     "shell.execute_reply.started": "2025-10-04T10:01:14.619902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def decode(outputs, blank=28):\n",
    "    \"\"\"Decode CTC output to text\"\"\"\n",
    "    _, max_indices = torch.max(outputs, dim=2)\n",
    "    output = []\n",
    "    for idx, indexes in enumerate(max_indices.transpose(0, 1)):\n",
    "        prev = -1\n",
    "        res = []\n",
    "        for index in indexes:\n",
    "            if index not in [prev, blank]:\n",
    "                res.append(index.item())\n",
    "            prev = index\n",
    "        output.append(res)\n",
    "    return output\n",
    "\n",
    "\n",
    "def validate(model, validation_loader, criterion, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        all_predicted_texts = []\n",
    "        all_true_texts = []\n",
    "        for batch_idx, _data in tqdm(enumerate(validation_loader)):\n",
    "            spectrograms, labels, input_lengths, label_lengths = _data\n",
    "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "            \n",
    "            output = model(spectrograms)\n",
    "            output = F.log_softmax(output, dim=2)\n",
    "            output = output.transpose(0, 1)  # Needed for CTCLoss\n",
    "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            decoded_outputs = decode(output)\n",
    "            predicted_texts = [text_transform.int_to_text(seq) for seq in decoded_outputs]\n",
    "            true_texts = [text_transform.int_to_text(label.tolist()) for label in labels]\n",
    "            \n",
    "            all_predicted_texts.extend(predicted_texts)\n",
    "            all_true_texts.extend(true_texts)\n",
    "            \n",
    "        avg_loss = total_loss / len(validation_loader)\n",
    "        avg_wer = np.mean([wer(ref, hyp) for ref, hyp in zip(all_true_texts, all_predicted_texts)])\n",
    "        avg_cer = np.mean([cer(ref, hyp) for ref, hyp in zip(all_true_texts, all_predicted_texts)])\n",
    "        \n",
    "        print(f\"Validation Loss: {avg_loss}\")\n",
    "        print(f\"Average WER: {avg_wer:.4f}\")\n",
    "        print(f\"Average CER: {avg_cer:.4f}\")\n",
    "        \n",
    "        return avg_loss, avg_wer, avg_cer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41a6f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:14.630556Z",
     "iopub.status.busy": "2025-10-04T10:01:14.630120Z",
     "iopub.status.idle": "2025-10-04T10:01:23.276022Z",
     "shell.execute_reply": "2025-10-04T10:01:23.275151Z",
     "shell.execute_reply.started": "2025-10-04T10:01:14.630520Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "root_path = '/kaggle/input/librispeech-clean'\n",
    "print(\"Note: Make sure to have the dataset in the input section of Kaggle\")\n",
    "\n",
    "\n",
    "train_dataset = torchaudio.datasets.LIBRISPEECH(root_path, url=\"train-clean-100\", download=False)\n",
    "test_dataset = torchaudio.datasets.LIBRISPEECH(root_path, url=\"test-clean\", download=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "\n",
    "waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = train_dataset[0]\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(waveform.t().numpy())\n",
    "ln = len(utterance)\n",
    "plt.title(f\"This is the waveform graph for the text:\\n {utterance[:ln//2-1]}\\n{utterance[ln//2-1:]}\")\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc9ceb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:23.277344Z",
     "iopub.status.busy": "2025-10-04T10:01:23.277083Z",
     "iopub.status.idle": "2025-10-04T10:01:23.283040Z",
     "shell.execute_reply": "2025-10-04T10:01:23.282234Z",
     "shell.execute_reply.started": "2025-10-04T10:01:23.277323Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                              batch_size=pipeline_params['batch_size'],\n",
    "                              shuffle=True,\n",
    "                              collate_fn=lambda x: data_processing(x, 'train'),\n",
    "                              **kwargs)\n",
    "\n",
    "test_loader = data.DataLoader(dataset=test_dataset,\n",
    "                             batch_size=pipeline_params['batch_size'],\n",
    "                             shuffle=True,\n",
    "                             collate_fn=lambda x: data_processing(x, 'valid'),\n",
    "                             **kwargs)\n",
    "\n",
    "print(f\"Train loader batches: {len(train_loader)}\")\n",
    "print(f\"Test loader batches: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68091a0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:23.284295Z",
     "iopub.status.busy": "2025-10-04T10:01:23.284021Z",
     "iopub.status.idle": "2025-10-04T10:01:26.443781Z",
     "shell.execute_reply": "2025-10-04T10:01:26.442961Z",
     "shell.execute_reply.started": "2025-10-04T10:01:23.284275Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.AdamW(model.parameters(), pipeline_params[\"learning_rate\"])\n",
    "criterion = nn.CTCLoss(blank=28).to(device)\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=pipeline_params[\"learning_rate\"],\n",
    "                                        steps_per_epoch=int(len(train_loader)),\n",
    "                                        epochs=pipeline_params[\"epochs\"],\n",
    "                                        anneal_strategy=\"linear\")\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Optimizer: {type(optimizer).__name__}\")\n",
    "print(f\"Criterion: {type(criterion).__name__}\")\n",
    "print(f\"Scheduler: {type(scheduler).__name__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c370c65",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T10:01:26.445742Z",
     "iopub.status.busy": "2025-10-04T10:01:26.445339Z",
     "iopub.status.idle": "2025-10-04T10:01:28.651944Z",
     "shell.execute_reply": "2025-10-04T10:01:28.650468Z",
     "shell.execute_reply.started": "2025-10-04T10:01:26.445718Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "data_len = len(train_loader.dataset)\n",
    "logging_idx = 0\n",
    "logging_freq = 100\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(pipeline_params[\"epochs\"]):\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), \n",
    "                       desc=f\"Epoch {epoch+1}/{pipeline_params['epochs']}\", unit=\"batches\")\n",
    "    \n",
    "    for batch_idx, _data in progress_bar:\n",
    "        spectrograms, labels, input_lengths, label_lengths = _data\n",
    "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(spectrograms)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1)\n",
    "\n",
    "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if logging_idx % logging_freq == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                epoch, \n",
    "                batch_idx * len(spectrograms), \n",
    "                data_len, 100. * batch_idx / len(train_loader), \n",
    "                loss.item()\n",
    "            ))\n",
    "        logging_idx += 1\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "print(\"Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb82385f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-04T10:01:28.653467Z",
     "iopub.status.idle": "2025-10-04T10:01:28.653948Z",
     "shell.execute_reply": "2025-10-04T10:01:28.653750Z",
     "shell.execute_reply.started": "2025-10-04T10:01:28.653720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_path = '/kaggle/working/transformer_speech_recognition_model.pth'\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to: {model_path}\")\n",
    "\n",
    "# Also save as .pt format for compatibility\n",
    "model_path_pt = '/kaggle/working/transformer_speech_recognition_model.pt'\n",
    "torch.save(model.state_dict(), model_path_pt)\n",
    "print(f\"Model also saved to: {model_path_pt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344560d4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-04T10:01:28.655649Z",
     "iopub.status.idle": "2025-10-04T10:01:28.655923Z",
     "shell.execute_reply": "2025-10-04T10:01:28.655808Z",
     "shell.execute_reply.started": "2025-10-04T10:01:28.655797Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Running validation on test set...\")\n",
    "validate(model, test_loader, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974334f8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-04T10:01:28.657661Z",
     "iopub.status.idle": "2025-10-04T10:01:28.657964Z",
     "shell.execute_reply": "2025-10-04T10:01:28.657842Z",
     "shell.execute_reply.started": "2025-10-04T10:01:28.657830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model loading and inference functions\n",
    "def load_model(model_path, device):\n",
    "    \"\"\"Load the transformer-enhanced model from saved state dict\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    loaded_model = SpeechRecognitionModel(\n",
    "        pipeline_params['n_cnn_layers'],\n",
    "        pipeline_params['n_rnn_layers'],\n",
    "        pipeline_params['rnn_dim'],\n",
    "        pipeline_params['n_class'],\n",
    "        pipeline_params['n_feats'],\n",
    "        pipeline_params['stride'],\n",
    "        pipeline_params['dropout'],\n",
    "        pipeline_params['n_heads'],\n",
    "        pipeline_params['n_transformer_layers'],\n",
    "        pipeline_params['transformer_dim']\n",
    "    ).to(device)\n",
    "    \n",
    "    loaded_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    loaded_model.eval()\n",
    "    \n",
    "    return loaded_model\n",
    "\n",
    "def speech_to_text(audio_path, model, device, text_transform, valid_audio_transform):\n",
    "    \"\"\"Convert speech audio file to text using the transformer-enhanced model\"\"\"\n",
    "    waveform, _ = torchaudio.load(audio_path)\n",
    "    waveform = valid_audio_transform(waveform)\n",
    "    waveform = waveform.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(waveform)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        output = output.transpose(0, 1)\n",
    "        decoded_output = decode(output)\n",
    "        predicted_text = text_transform.int_to_text(decoded_output[0])\n",
    "    \n",
    "    return predicted_text\n",
    "\n",
    "print(\"Model loading and inference functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6c4981-1020-482a-8781-b2da9d8322b4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-10-04T10:01:28.659009Z",
     "iopub.status.idle": "2025-10-04T10:01:28.659415Z",
     "shell.execute_reply": "2025-10-04T10:01:28.659231Z",
     "shell.execute_reply.started": "2025-10-04T10:01:28.659214Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/kaggle/working/transformer_speech_recognition_model.pt'))\n",
    "text_transform = TextPreProcess()\n",
    "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
    "\n",
    "audio_path = '/kaggle/input/librispeech/LibriSpeech/dev-clean/174/50561/174-50561-0002.wav'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "predicted_text = speech_to_text(audio_path, model, device, text_transform, valid_audio_transforms)\n",
    "print(f\"Predicted Text: {predicted_text}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 230420,
     "sourceId": 492337,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 1670098,
     "sourceId": 2739456,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4844866,
     "sourceId": 8182822,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
